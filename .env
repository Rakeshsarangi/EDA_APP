# Ollama Configuration
# The model to use for LLM operations
# Options: 
# - For smaller/faster: "llama2:7b", "mistral:7b", "phi:latest"
# - For better quality: "llama2:13b", "mixtral:8x7b", "solar:latest"
# - For best quality: "llama2:70b", "mixtral:8x22b"
# - Custom models: "codellama:latest", "neural-chat:latest", etc.
OLLAMA_MODEL=gpt-oss:20b

# Ollama API endpoint
# Default is localhost, change if Ollama is running on a different machine
OLLAMA_BASE_URL=http://localhost:11434


# Security settings
MAX_FILE_SIZE_MB = 100

# Analysis settings
MAX_ROWS_FOR_ANALYSIS = 1000000
SAMPLE_SIZE_FOR_LLM = 100

# Visualization settings
DEFAULT_PLOT_HEIGHT = 400
DEFAULT_PLOT_WIDTH = 600
COLOR_PALETTE = "viridis"